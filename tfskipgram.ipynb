{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word2Vec: Skipgram Model\n",
    "-------------------------------------\n",
    "\n",
    "In this example, we will download and preprocess the movie review data.\n",
    "\n",
    "From this data set we will compute/fit the skipgram model of the Word2Vec Algorithm\n",
    "\n",
    "Skipgram: based on predicting the surrounding words from the\n",
    "\n",
    "Ex sentence \"the cat in the hat\"\n",
    " - context word:  [\"hat\"]\n",
    " - target words: [\"the\", \"cat\", \"in\", \"the\"]\n",
    " - context-target pairs: (\"hat\", \"the\"), (\"hat\", \"cat\"), (\"hat\", \"in\"), (\"hat\", \"the\")\n",
    "\n",
    "We start by loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Start a computational graph session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Declare model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # How many sets of words to train on at once.\n",
    "embedding_size = 100    # The embedding size of each word to train.\n",
    "vocabulary_size = 2000 # How many words we will consider for training.\n",
    "generations = 100000    # How many iterations we will perform the training on.\n",
    "print_loss_every = 1000  # Print the loss every so many iterations\n",
    "\n",
    "num_sampled = int(batch_size/2) # Number of negative examples to sample.\n",
    "window_size = 5         # How many words to consider left and right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will remove stop words and create a test validation set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Declare stop words\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# We pick five test words. We are expecting synonyms to appear\n",
    "print_valid_every = 10000\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']\n",
    "# Later we will have to transform these into indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we load the movie review data.  We check if the data was downloaded, and not, download and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_movie_data():\n",
    "    save_folder_name = 'temp'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
    "\n",
    "    # Check if files are already downloaded\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "\n",
    "        # Save tar.gz file\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        # Extract tar.gz file into temp folder\n",
    "        tar = tarfile.open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path='temp')\n",
    "        tar.close()\n",
    "\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    pos_data = [x.rstrip() for x in pos_data]\n",
    "\n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    neg_data = [x.rstrip() for x in neg_data]\n",
    "    \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    \n",
    "    return(texts, target)\n",
    "\n",
    "\n",
    "texts, target = load_movie_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we create a function that normalizes/cleans the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Normalize text\n",
    "def normalize_text(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)\n",
    "    \n",
    "texts = normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the normalized movie reviews, we now build a dictionary of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build dictionary of words\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    # Turn sentences (list of strings) into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    \n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [['RARE', -1]]\n",
    "    \n",
    "    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    \n",
    "    # Now create the dictionary\n",
    "    word_dict = {}\n",
    "    # For each word, that we want in the dictionary, add it, then make it\n",
    "    # the value of the prior dictionary length\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the above dictionary, we can turn text data into lists of integers from such dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1490, 28, 940, 205, 359]\n"
     ]
    }
   ],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    # Initialize the returned data\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        # For each word, either use selected index or rare word index\n",
    "        for word in sentence.split(' '):\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)\n",
    "\n",
    "# Build our data set and dictionaries\n",
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us now build a function that will generate random data points from our text and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate data randomly (N words behind, target, N words ahead)\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    print(sentences[0])\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return(batch_data, label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we define our model and placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(2000, 100) dtype=float32_ref>\n<tf.Variable 'Variable_5:0' shape=(2000,) dtype=float32_ref>\nTensor(\"Placeholder_3:0\", shape=(100, 1), dtype=int32)\nTensor(\"embedding_lookup_1:0\", shape=(100, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding:\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)\n",
    "\n",
    "print(nce_weights)\n",
    "print(nce_biases)\n",
    "print(y_target)\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is our loss function, optimizer, cosine similarity, and initialization of the model variables.\n",
    "\n",
    "For the loss function we will minimize the average of the NCE loss (noise-contrastive estimation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_init = sess.run(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can train our skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n[527, 0, 0, 0, 33, 0, 214, 147, 16, 0, 7, 0, 1383, 0, 0, 1509, 0, 754, 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-6fd4979a4e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_x_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_inputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-f2053964a5fa>\u001b[0m in \u001b[0;36mgenerate_batch_data\u001b[0;34m(sentences, batch_size, window_size, method)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# select random sentence to start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrand_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Generate consecutive windows to look at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mwindow_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrand_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Run the skip gram model.\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                score = sim[j,nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
